{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('py38': conda)",
   "display_name": "Python 3.8.3 64-bit ('py38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1cb9a1c850fd1d16c5b98054247a74b7b7a12849bcfa00436ba202c2a9e2bbb2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_datamodule import NLPDataModule\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pl_bolts.models import LogisticRegression\n",
    "from pytorch_lightning import Trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataModuleHF(NLPDataModule):\n",
    "\n",
    "    def __init__(self, max_len: int = 500, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    # NOTE: this can be then done automatically\n",
    "    def pipeline(self, data, stage=None):\n",
    "        data[\"text\"] = self.normalization(data[\"text\"])\n",
    "        data[\"text\"] = self.tokenization(data[\"text\"])\n",
    "        data[\"text\"] = self.cleaning(data[\"text\"])\n",
    "        if stage == \"test\":\n",
    "            data[\"text\"] = super().numericalization(\n",
    "                data[\"text\"], max_len=self.max_len, pad=self.word2index[\"<pad>\"]\n",
    "            )\n",
    "        return data\n",
    "    \n",
    "    # NOTE: Here I am forced to rewrite because I need to output a dict\n",
    "    # but there may be a better solution\n",
    "    def numericalization(self, data, max_len, pad):\n",
    "        data[\"text\"] = super().numericalization(data[\"text\"], max_len, pad)\n",
    "        return data\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        if stage == 'fit' or stage is None:\n",
    "            ds = load_dataset(\"imdb\", split=\"train\")\n",
    "            self.num_classes = ds.features[\"label\"].num_classes\n",
    "            ds = ds.map(self.pipeline, fn_kwargs={\"stage\": stage})\n",
    "\n",
    "            # only after the text is clean I want to build vocab\n",
    "            if self.vocab is None:\n",
    "                self.build_vocab(ds[\"text\"])\n",
    "            ds = ds.map(self.numericalization, fn_kwargs={\"max_len\": self.max_len, \"pad\": self.word2index[\"<pad>\"]})\n",
    "            ds = ds.train_test_split(test_size=.2)\n",
    "\n",
    "            self.train_ds = ds[\"train\"]\n",
    "            self.val_ds = ds[\"test\"]\n",
    "            self.train_ds.set_format(type='torch', columns=['text', 'label'])\n",
    "            self.val_ds.set_format(type='torch', columns=['text', 'label'])\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.test_ds = load_dataset(\"imdb\", split=\"test\")\n",
    "            self.test_ds = self.test_ds.map(self.pipeline, fn_kwargs={\"stage\": stage})\n",
    "            self.test_ds.set_format(type='torch', columns=['text', 'label'])\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return DataLoader(self.validation_ds, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size, collate_fn=self.collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batches):\n",
    "        x = torch.stack([batch[\"text\"] for batch in batches]).float()\n",
    "        y = torch.stack([batch[\"label\"] for batch in batches])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset imdb (/Users/49796/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Loading cached processed dataset at /Users/49796/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-1bbed78b80c4854d.arrow\n",
      "Building vocab: 100%|██████████| 25000/25000 [00:00<00:00, 43807.71it/s]\n",
      "Loading cached processed dataset at /Users/49796/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-0a76965b8264aa9a.arrow\n"
     ]
    }
   ],
   "source": [
    "dm = TextClassificationDataModuleHF()\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | linear | Linear | 1 K   \n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s, loss=2210.080, v_num=6, train_ce_loss=2.21e+3]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = LogisticRegression(input_dim=dm.max_len, num_classes=dm.num_classes)\n",
    "trainer = Trainer(fast_dev_run=True)\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset imdb (/Users/49796/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Testing: 0it [00:00, ?it/s]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': tensor(0.1562),\n",
      " 'test_ce_loss': tensor(6827.8779),\n",
      " 'test_loss': tensor(6827.8779)}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test_ce_loss': 6827.8779296875,\n",
       "  'test_acc': 0.15625,\n",
       "  'test_loss': 6827.8779296875}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "dm.setup(stage=\"test\")\n",
    "trainer.test(model, datamodule=dm)"
   ]
  }
 ]
}